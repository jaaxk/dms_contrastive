# -*- coding: utf-8 -*-
"""dms_cosine_jv.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PJEilEw4yQiz4Y8PgGoiay8RDkPvwIsQ
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.linear_model import LogisticRegression
import warnings
import esm
from tqdm import tqdm
import pickle
import os
import argparse
warnings.filterwarnings('ignore')

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

def parse_args():
    parser = argparse.ArgumentParser(
        prog='DMS_contrastive',
        description='DMS contrastive learning pipeline'
    )

    parser.add_argument('--run_name', type=str, required=True)

    #ML hyperparams
    parser.add_argument('--batch_size', type=int, default=32)
    parser.add_argument('--learning_rate', type=float, default=0.0001)
    parser.add_argument('--num_epochs', type=int, default=200)
    parser.add_argument('--patience', type=int, default=50)
    parser.add_argument('--distance_metric', choices=['cosine', 'euclidean'], default='cosine')
    parser.add_argument('--use_learnable', type=bool, default=False)

    #model hyperparams
    parser.add_argument('--hidden_dims', type=list, default=[512, 256, 128])
    parser.add_argument('--normalize_output', type=bool, default=True)
    parser.add_argument('--freeze_esm', action='store_true', default=False)

    #data
    #parser.add_argument('--embeddings_path', default='dms_data/embeddings/Stability/embeddings_layer11_mean.pkl')
    parser.add_argument('--data_path', default='dms_data/datasets/Stability.csv')
    parser.add_argument('--split_by_gene', choices=[True, False], default=True, help='train/test split by gene rather than variant')
    parser.add_argument('--base_results_dir', default='results')
    parser.add_argument('--homologous_batch', action='store_true', help='each batch contains a single gene, no cross-gene pairs', default=False)

    args = parser.parse_args()
    return args

args = parse_args()

#EMBEDDINGS_PATH = args.embeddings_path
DATA_PATH = args.data_path
RUN_NAME = args.run_name

#params
BATCH_SIZE = args.batch_size
LEARNING_RATE = args.learning_rate
NUM_EPOCHS = args.num_epochs
PATIENCE = args.patience
DISTANCE_METRIC = args.distance_metric
USE_LEARNABLE = args.use_learnable

# Model architecture
HIDDEN_DIMS = args.hidden_dims
NORMALIZE_OUTPUT = args.normalize_output

#results dir
RESULTS_DIR = f'{args.base_results_dir}/{args.run_name}'
os.makedirs(RESULTS_DIR, exist_ok=True)
os.makedirs(f'{RESULTS_DIR}/training_figs', exist_ok=True)
RESULTS_FILE = f'{args.base_results_dir}/results.csv'
if not os.path.exists(RESULTS_FILE):
    with open(RESULTS_FILE, 'w') as f:
        f.write('run_name,test_loss,test_acc,test_precision,test_recall,test_f1,test_auc, baseline_acc, baseline_f1\n')

class DMSContrastiveDataset(Dataset):
    def __init__(self, sequences, quartiles, dms_scores, genes):
        self.sequences = sequences
        self.quartiles = quartiles
        self.dms_scores = dms_scores
        #self.seq_to_embedding = seq_to_embedding
        self.genes = genes

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        seq = self.sequences[idx]
        quartile = self.quartiles[idx]
        dms_score = self.dms_scores[idx]
        gene = self.genes[idx]

        #get precomputed embedding
        #emb = self.seq_to_embedding[seq]

        return {
            'quartile': quartile,
            'dms_score': dms_score,
            'sequence': seq,
            'gene': gene
        }


class GeneAwareDataLoader:
    def __init__(self, dataset, batch_size=16, shuffle=True, balance_quartiles=True):
        self.dataset = dataset
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.balance_quartiles = balance_quartiles

        print("Using homologous batch sampling - each batch will contain variants from the same gene")
        #group indices by gene and quartile
        self.gene_groups = {}
        for i, gene in enumerate(dataset.genes):
            if gene not in self.gene_groups:
                self.gene_groups[gene] = {'high': [], 'low': []}
            quartile = dataset.quartiles[i]
            self.gene_groups[gene][quartile].append(i)

        print(f"Gene-aware loader created for {len(self.gene_groups)} genes")
        for gene, quartile_dict in list(self.gene_groups.items())[:3]:
            print(f"  {gene}: {len(quartile_dict['high'])} high, {len(quartile_dict['low'])} low")

    def __iter__(self):
        all_batches = []

        for gene, quartile_dict in self.gene_groups.items():
            high_indices = quartile_dict['high'].copy()
            low_indices = quartile_dict['low'].copy()

            if self.shuffle:
                np.random.shuffle(high_indices)
                np.random.shuffle(low_indices)

            if self.balance_quartiles and len(high_indices) > 0 and len(low_indices) > 0:
                half_batch = self.batch_size // 2

                num_batches = min(len(high_indices) // half_batch, len(low_indices) // half_batch)

                for i in range(num_batches):
                    batch_high = high_indices[i*half_batch:(i+1)*half_batch]
                    batch_low = low_indices[i*half_batch:(i+1)*half_batch]
                    batch_indices = batch_high + batch_low

                    if self.shuffle:
                        np.random.shuffle(batch_indices)

                    batch = [self.dataset[idx] for idx in batch_indices]
                    all_batches.append(batch)
            else:
                #fall back to unbalanced batching if needed
                all_indices = high_indices + low_indices
                if self.shuffle:
                    np.random.shuffle(all_indices)

                for i in range(0, len(all_indices), self.batch_size):
                    batch_indices = all_indices[i:i+self.batch_size]
                    if len(batch_indices) >= 2:
                        batch = [self.dataset[idx] for idx in batch_indices]
                        all_batches.append(batch)

        if self.shuffle:
            np.random.shuffle(all_batches)

        for batch in all_batches:
            yield self._collate_batch(batch)

    def _collate_batch(self, batch):
        #embeddings = torch.stack([item['embedding'] for item in batch])
        quartiles = [item['quartile'] for item in batch]
        dms_scores = [item['dms_score'] for item in batch]
        sequences = [item['sequence'] for item in batch]
        genes = [item['gene'] for item in batch]

        return {
            'quartiles': quartiles,
            'dms_scores': dms_scores,
            'sequences': sequences,
            'genes': genes
        }

    def __len__(self):
        total_samples = len(self.dataset)
        return total_samples // self.batch_size

class DataLoader():
    def __init__(self, dataset, batch_size=16, shuffle=True, balance_quartiles=True):
        self.dataset = dataset
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.balance_quartiles = balance_quartiles

        print('Using basic data loader - no gene grouping')
        
        # Standard behavior - no gene grouping
        self.indices = list(range(len(dataset)))
        if self.shuffle:
            np.random.shuffle(self.indices)

    def __iter__(self):
        # Separate indices by quartile
        high_indices = [i for i, q in enumerate(self.dataset.quartiles) if q == 'high']
        low_indices = [i for i, q in enumerate(self.dataset.quartiles) if q == 'low']
        
        if self.shuffle:
            np.random.shuffle(high_indices)
            np.random.shuffle(low_indices)
        
        # Determine the number of batches we can make
        half_batch = self.batch_size // 2
        num_batches = min(len(high_indices), len(low_indices)) // half_batch
        
        for i in range(num_batches):
            # Take equal number of high and low quartile samples
            batch_high = high_indices[i*half_batch:(i+1)*half_batch]
            batch_low = low_indices[i*half_batch:(i+1)*half_batch]
            batch_indices = batch_high + batch_low
            
            # Shuffle the combined batch
            if self.shuffle:
                np.random.shuffle(batch_indices)
                
            batch = [self.dataset[idx] for idx in batch_indices]
            yield self._collate_batch(batch)

    def _collate_batch(self, batch):
        #embeddings = torch.stack([item['embedding'] for item in batch])
        quartiles = [item['quartile'] for item in batch]
        dms_scores = [item['dms_score'] for item in batch]
        sequences = [item['sequence'] for item in batch]
        genes = [item['gene'] for item in batch]

        return {
            'quartiles': quartiles,
            'dms_scores': dms_scores,
            'sequences': sequences,
            'genes': genes
        }

    def __len__(self):
        return len(self.indices) // self.batch_size

class ContrastiveNetwork(nn.Module):
    def __init__(self, esm_model, input_dim=1280, hidden_dims=[512, 256, 128], normalize_output=False):
        super(ContrastiveNetwork, self).__init__()

        self.esm = esm_model

        layers = []
        prev_dim = input_dim

        for i, hidden_dim in enumerate(hidden_dims):
            layers.append(nn.Linear(prev_dim, hidden_dim))

            if i < len(hidden_dims) - 1:
                layers.append(nn.LayerNorm(hidden_dim))
                layers.append(nn.ReLU())

            prev_dim = hidden_dim

        self.projection = nn.Sequential(*layers)
        self.normalize_output = normalize_output

    def forward(self, tokens):
        if args.freeze_esm:
            with torch.no_grad():
                out = self.esm(tokens, repr_layers=[33])  # Use the last layer
        else:
            out = self.esm(tokens, repr_layers=[33])  # Use the last layer

        reps = out["representations"][33]
        embs = reps.mean(dim=1) #mean pool
        x = self.projection(embs)  # Use mean pooled representations
        if self.normalize_output:
            #l2 normalize
            x = nn.functional.normalize(x, p=2, dim=-1)
        return x

class ContrastiveLoss(nn.Module):
    def __init__(self, distance_metric="cosine", use_learnable=True):
        super(ContrastiveLoss, self).__init__()
        self.distance_metric = distance_metric
        self.use_learnable = use_learnable

        if use_learnable:
            if distance_metric == "euclidean":
                self.alpha = nn.Parameter(torch.tensor(-10.0))
            else:  #cosine
                #positive: high similarity → high probability
                self.alpha = nn.Parameter(torch.tensor(10.0))
            self.beta = nn.Parameter(torch.tensor(0.0))

    def forward(self, embeddings, quartiles):
        batch_size = embeddings.shape[0]
        device = embeddings.device

        if self.distance_metric == "euclidean":
            norms = (embeddings ** 2).sum(dim=1, keepdim=True)  # (B, 1)
            distance_matrix = norms + norms.t() - 2 * torch.mm(embeddings, embeddings.t())
            distance_matrix = torch.sqrt(torch.clamp(distance_matrix, min=1e-8))  # (B, B)

            if self.use_learnable:
                logit_matrix = self.alpha * distance_matrix + self.beta
                similarity_matrix = torch.sigmoid(logit_matrix)
            else:
                similarity_matrix = torch.exp(-distance_matrix)
                similarity_matrix = torch.clamp(similarity_matrix, min=1e-7, max=1-1e-7)

        elif self.distance_metric == "cosine":
            embeddings_norm = nn.functional.normalize(embeddings, p=2, dim=1)  # (B, D)

            #pairwise cosine similarity: normalized dot products
            cosine_sim_matrix = torch.mm(embeddings_norm, embeddings_norm.t())  # (B, B)

            #distance = 1 - cosine_sim
            distance_matrix = 1 - cosine_sim_matrix

            if self.use_learnable:
                logit_matrix = self.alpha * cosine_sim_matrix + self.beta
                similarity_matrix = torch.sigmoid(logit_matrix)
            else:
                # Fixed transformation: map [-1, 1] to [0, 1]
                similarity_matrix = (cosine_sim_matrix * 0.5) + 0.5
                similarity_matrix = torch.clamp(similarity_matrix, min=1e-7, max=1-1e-7)
        else:
            raise ValueError(f"Unknown distance metric: {self.distance_metric}")

        #create label matrix: 1 if same quartile, 0 if different
        quartile_tensor = torch.tensor([1 if q == 'high' else 0 for q in quartiles],
                                       dtype=torch.long, device=device)
        label_matrix = (quartile_tensor.unsqueeze(0) == quartile_tensor.unsqueeze(1)).float()

        #extract upper triangle (excluding diagonal) for pairs
        upper_triangle_mask = torch.triu(torch.ones_like(label_matrix), diagonal=1).bool()

        similarities = similarity_matrix[upper_triangle_mask]
        labels = label_matrix[upper_triangle_mask]
        distances = distance_matrix[upper_triangle_mask]

        #compute loss
        loss = nn.BCELoss()(similarities, labels)

        all_similarities = similarities.detach().cpu().tolist()
        all_distances = distances.detach().cpu().tolist()
        all_labels = labels.cpu().tolist()

        return loss, all_similarities, all_distances, all_labels

def load_embeddings(embeddings_path):
    print(f"Loading embeddings from {embeddings_path}")

    if not os.path.exists(embeddings_path):
        raise FileNotFoundError(f"Embeddings file not found: {embeddings_path}")

    with open(embeddings_path, 'rb') as f:
        data = pickle.load(f)

    embeddings = data['embeddings']
    sequences = data['sequences']

    print(f"Loaded {len(sequences)} embeddings")
    print(f"Embedding shape: {embeddings[0].shape if len(embeddings) > 0 else 'N/A'}")

    #sequence to embedding mapping
    seq_to_embedding = {seq: emb for seq, emb in zip(sequences, embeddings)}

    return seq_to_embedding

def load_and_preprocess_data(data_path):
    print(f"Loading data from {data_path}")
    df = pd.read_csv(data_path)

    df = df.dropna(subset=['DMS_score', 'mutated_sequence', 'filename'])

    #get unique genes
    unique_genes = df['filename'].unique()
    print(f"Found {len(unique_genes)} unique genes")

    #process each gene separately (gene-aware quartile calculation)
    all_processed_dfs = []

    for gene in unique_genes:
        gene_df = df[df['filename'] == gene].copy()

        if len(gene_df) < 10:
            print(f"Skipping gene {gene} (only {len(gene_df)} samples)")
            continue

        #calculate quartiles for THIS gene
        q25 = gene_df['DMS_score'].quantile(0.25)
        q75 = gene_df['DMS_score'].quantile(0.75)

        #filter to top 75% and bottom 25% for this gene
        top_75 = gene_df[gene_df['DMS_score'] >= q75].copy()
        bottom_25 = gene_df[gene_df['DMS_score'] <= q25].copy()

        #add quartile labels
        top_75['quartile'] = 'high'
        bottom_25['quartile'] = 'low'

        #combine for this gene
        gene_processed = pd.concat([top_75, bottom_25], ignore_index=True)
        all_processed_dfs.append(gene_processed)

        print(f"  {gene}: {len(gene_processed)} samples (high: {len(top_75)}, low: {len(bottom_25)})")

    #combine all genes
    combined_df = pd.concat(all_processed_dfs, ignore_index=True)
    print(f"\n Total processed samples: {len(combined_df)}")

    return combined_df

def create_train_test_split(df, split_by_gene=True, test_size=0.2):
    print("\nCreating gene-aware train/test split...")

    #get unique sequences
    all_sequences = df['mutated_sequence'].unique()
    print(f"Total unique sequences: {len(all_sequences)}")

    #get unique genes
    all_genes = df['uniprot_id'].unique()
    print(f"Total unique genes: {len(all_genes)}")

    if split_by_gene:
      print('Splitting by gene...')
      train_genes, test_genes = train_test_split(
        all_genes, test_size=test_size, random_state=42
      )

      print(f"Train genes: {len(train_genes)}")
      print(f"Test genes: {len(test_genes)}")

      #create train/test dataframes
      train_df = df[df['uniprot_id'].isin(train_genes)].copy()
      test_df = df[df['uniprot_id'].isin(test_genes)].copy()
      
      train_sequences = train_df['mutated_sequence']
      test_sequences = test_df['mutated_sequence']

    else:
      print('Splitting by sequence...')
      #split sequences
      train_sequences, test_sequences = train_test_split(
          all_sequences, test_size=test_size, random_state=42
      )

      print(f"Train sequences: {len(train_sequences)}")
      print(f"Test sequences: {len(test_sequences)}")

      #create train/test dataframes
      train_df = df[df['mutated_sequence'].isin(train_sequences)].copy()
      test_df = df[df['mutated_sequence'].isin(test_sequences)].copy()

      #get genes
      train_genes = train_df['uniprot_id']
      test_genes = test_df['uniprot_id']

    print(f"Train samples: {len(train_df)}")
    print(f"Test samples: {len(test_df)}")

    #verify no GENE overlap
    overlap = set(train_genes).intersection(set(test_genes))
    if len(overlap) == 0:
        print("✅ No gene overlap between train and test - proper held-out test!")
    else:
        print(f"⚠️  Warning: {len(overlap)} genes overlap")

    #verify no SEQUENCE overlap
    overlap = set(train_sequences).intersection(set(test_sequences))
    if len(overlap) == 0:
        print("✅ No sequence overlap between train and test - proper held-out test!")
    else:
        print(f"⚠️  Warning: {len(overlap)} sequences overlap")

    return train_df, test_df

def train_epoch(projection_net, loss_fn, dataloader, optimizer, device, n_clusters=2):
    projection_net.train()

    total_loss = 0
    all_similarities = []
    all_labels = []
    all_distances = []
    all_projections = []
    all_quartiles = []
    num_batches = 0

    for batch in dataloader:
        optimizer.zero_grad()

        sequences = batch['sequences'].to(device)
        quartiles = batch['quartiles']

        #project embeddings
        projected = projection_net(sequences)
        all_projections.extend(projected.cpu().detach().numpy())

        #compute loss
        loss, similarities, distances, labels = loss_fn(projected, quartiles)

        #backward pass
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        num_batches += 1

        all_similarities.extend(similarities)
        all_distances.extend(distances)
        all_labels.extend(labels)
        all_quartiles.extend(quartiles)

    return total_loss / max(num_batches, 1), all_similarities, all_labels, all_distances, all_quartiles, all_projections

def evaluate_model(projection_net, loss_fn, dataloader, device, n_clusters=2):
    projection_net.eval()

    all_similarities = []
    all_labels = []
    all_distances = []
    all_kmeans_labels = []
    all_quartiles = []
    all_projections = []
    total_loss = 0
    num_batches = 0

    with torch.no_grad():
        for batch in dataloader:
            sequences = batch['sequences'].to(device)
            quartiles = batch['quartiles']

            #project embeddings
            projected = projection_net(sequences)
            all_projections.extend(projected.cpu().detach().numpy())

            #compute loss
            loss, similarities, distances, labels = loss_fn(projected, quartiles)

            total_loss += loss.item()
            num_batches += 1

            all_similarities.extend(similarities)
            all_distances.extend(distances)
            all_labels.extend(labels)
            all_quartiles.extend(quartiles)

    avg_loss = total_loss / max(num_batches, 1)
    return avg_loss, all_similarities, all_labels, all_distances, all_quartiles, all_projections

def kmeans_metrics(projections, quartiles, n_clusters=2):
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    kmeans.fit(projections)
    kmeans_labels = kmeans.labels_

    quartiles = [1 if q == 'high' else 0 for q in quartiles]

    accuracy = max(accuracy_score(quartiles, kmeans_labels), 1 - accuracy_score(quartiles, kmeans_labels))
    precision = max(precision_score(quartiles, kmeans_labels), 1 - precision_score(quartiles, kmeans_labels))
    recall = max(recall_score(quartiles, kmeans_labels), 1 - recall_score(quartiles, kmeans_labels))
    f1 = max(f1_score(quartiles, kmeans_labels), 1 - f1_score(quartiles, kmeans_labels))

    return accuracy, precision, recall, f1

def logreg_metrics(train_projections, train_quartiles, test_projections, test_quartiles, sample_train=1000):
    # Sample data for efficiency
    if len(train_projections) > sample_train:
        train_indices = np.random.choice(len(train_projections), sample_train, replace=False)
        train_proj_sample = [train_projections[i] for i in train_indices]
        train_quartile_sample = [train_quartiles[i] for i in train_indices]
    else:
        train_proj_sample = train_projections
        train_quartile_sample = train_quartiles
    
    # Convert to numpy arrays
    X_train = np.array(train_proj_sample)
    y_train = np.array([1 if q == 'high' else 0 for q in train_quartile_sample])
    
    X_test = np.array(test_projections)
    y_test = np.array([1 if q == 'high' else 0 for q in test_quartiles])
    
    # Train logistic regression
    clf = LogisticRegression(random_state=42)
    clf.fit(X_train, y_train)
    
    # Predictions
    y_pred = clf.predict(X_test)
    
    # Metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    
    return accuracy, precision, recall, f1

def plot_training_metrics(train_losses, val_losses, train_accs, val_accs):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

    #loss plot
    ax1.plot(train_losses, label='Train Loss', color='blue')
    ax1.plot(val_losses, label='Val Loss', color='red')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.set_title('Training and Validation Loss (Cosine Similarity)')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    #accuracy plot
    ax2.plot(train_accs, label='Train Accuracy', color='blue')
    ax2.plot(val_accs, label='Val Accuracy', color='red')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy')
    ax2.set_title('Training and Validation Accuracy')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(f'{RESULTS_DIR}/training_metrics.png')

def plot_distance_clustering(distances, labels, title="Distance Clustering", final=False):
    similar_distances = [d for d, l in zip(distances, labels) if l == 1]
    dissimilar_distances = [d for d, l in zip(distances, labels) if l == 0]

    plt.figure(figsize=(12, 5))

    #histogram comparison
    plt.subplot(1, 2, 1)
    plt.hist(similar_distances, bins=30, alpha=0.7, label='Similar (1)', color='blue', density=True)
    plt.hist(dissimilar_distances, bins=30, alpha=0.7, label='Dissimilar (0)', color='red', density=True)
    plt.xlabel('Cosine Distance (1 - cosine_sim)')
    plt.ylabel('Density')
    plt.title(f'{title} - Distribution')
    plt.legend()
    plt.grid(True, alpha=0.3)

    #box plot comparison
    plt.subplot(1, 2, 2)
    data_to_plot = [similar_distances, dissimilar_distances]
    box_plot = plt.boxplot(data_to_plot, labels=['Similar (1)', 'Dissimilar (0)'],
                          patch_artist=True, showfliers=True)
    box_plot['boxes'][0].set_facecolor('lightblue')
    box_plot['boxes'][1].set_facecolor('lightcoral')
    plt.ylabel('Cosine Distance')
    plt.title(f'{title} - Box Plot')
    plt.grid(True, alpha=0.3)

    #add statistics
    similar_mean = np.mean(similar_distances)
    dissimilar_mean = np.mean(dissimilar_distances)
    separation = dissimilar_mean - similar_mean

    plt.figtext(0.5, 0.02,
                f'Similar mean: {similar_mean:.4f} | Dissimilar mean: {dissimilar_mean:.4f} | Separation: {separation:.4f}',
                ha='center', fontsize=10, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray"))

    plt.tight_layout()
    if final:
        plt.savefig(f'{RESULTS_DIR}/{title}.png')
    else:
        plt.savefig(f'{RESULTS_DIR}/training_figs/{title}.png')

    return similar_mean, dissimilar_mean, separation

def visualize_embeddings_tsne(embeddings, labels, title="t-SNE Visualization"):
    print(f"Creating t-SNE visualization: {title}")

    #reduce dimensionality
    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(embeddings)-1))
    embeddings_2d = tsne.fit_transform(embeddings.cpu().numpy())

    plt.figure(figsize=(10, 8))
    scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1],
                         c=labels, cmap='viridis', alpha=0.7, s=50)
    plt.colorbar(scatter, label='DMS Score')
    plt.title(title)
    plt.xlabel('t-SNE 1')
    plt.ylabel('t-SNE 2')
    plt.tight_layout()
    plt.savefig(f'{RESULTS_DIR}/{title}.png')

def main():
    print("="*80)
    print(f"Running Contrastive Learning for {RUN_NAME}")
    print("="*80)
    #print(f"Embeddings Path: {EMBEDDINGS_PATH}")
    print(f"Data Path: {DATA_PATH}")
    print(f"Device: {device}")
    print(f"Distance Metric: {DISTANCE_METRIC}")
    print(f"Learnable Transformation: {USE_LEARNABLE}")
    print(f"Output Normalization: {NORMALIZE_OUTPUT}")
    print("="*80)

    """try:
        from google.colab import drive
        drive.mount('/content/drive')
    except:
        print("Not in Colab environment, skipping drive mount")"""

    #load embeddings
    #seq_to_embedding = load_embeddings(EMBEDDINGS_PATH)

    #load and preprocess data
    df = load_and_preprocess_data(DATA_PATH)

    #filter to sequences that have embeddings
    #available_sequences = set(seq_to_embedding.keys())
    #df = df[df['mutated_sequence'].isin(available_sequences)]
    #print(f"\n Filtered to {len(df)} samples with available embeddings")

    #create train/test split
    train_df, test_df = create_train_test_split(df, test_size=0.2)

    #extract data
    train_seqs = train_df['mutated_sequence'].tolist()
    train_quarts = train_df['quartile'].tolist()
    train_dms = train_df['DMS_score'].tolist()
    train_genes = train_df['filename'].tolist()

    test_seqs = test_df['mutated_sequence'].tolist()
    test_quarts = test_df['quartile'].tolist()
    test_dms = test_df['DMS_score'].tolist()
    test_genes = test_df['filename'].tolist()

    #create datasets
    train_dataset = DMSContrastiveDataset(train_seqs, train_quarts, train_dms,
                                          train_genes)
    test_dataset = DMSContrastiveDataset(test_seqs, test_quarts, test_dms,
                                         test_genes)

    #create data loaders
    if args.homologous_batch:
        train_loader = GeneAwareDataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
        test_loader = GeneAwareDataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)
    else:
        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

    print(f"\n Data loaders created")
    print(f"   Train batches: ~{len(train_loader)}")
    print(f"   Test batches: ~{len(test_loader)}")

    #check first batch
    first_batch = next(iter(train_loader))
    quartiles_in_batch = first_batch['quartiles']
    genes_in_batch = first_batch['genes']
    high_count = sum(1 for q in quartiles_in_batch if q == 'high')
    low_count = sum(1 for q in quartiles_in_batch if q == 'low')
    unique_genes = len(set(genes_in_batch))
    print(f"\n First batch check:")
    print(f"   Batch size: {len(quartiles_in_batch)}")
    print(f"   High: {high_count}, Low: {low_count}")
    print(f"   Unique genes in batch: {unique_genes} (should be 1 for gene-aware)")

    #init model
    esm_model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()
    batch_converter = alphabet.get_batch_converter()
    projection_net = ContrastiveNetwork(
        esm_model=esm_model,
        input_dim=1280,
        hidden_dims=HIDDEN_DIMS,
        normalize_output=NORMALIZE_OUTPUT
    ).to(device)

    loss_fn = ContrastiveLoss(
        distance_metric=DISTANCE_METRIC,
        use_learnable=USE_LEARNABLE
    )

    optimizer = optim.Adam(projection_net.parameters(), lr=LEARNING_RATE)

    print(f"\n Model initialized")
    print(f"   Parameters: {sum(p.numel() for p in projection_net.parameters()):,}")
    if USE_LEARNABLE:
        print(f"   Learnable alpha: {loss_fn.alpha.item():.4f}")
        print(f"   Learnable beta: {loss_fn.beta.item():.4f}")

    #train loop
    train_losses = []
    val_losses = []
    train_accs = []
    val_accs = []

    best_val_loss = float('inf')
    patience_counter = 0
    best_model_state = None
    
    print(f"\n{'='*80}")
    print("STARTING TRAINING")
    print(f"{'='*80}\n")

    for epoch in tqdm(range(NUM_EPOCHS), desc="Training Progress"):
        #train
        train_loss, train_similarities, train_labels_epoch, train_dists, train_quartiles, train_projections = train_epoch(
            projection_net, loss_fn, train_loader, optimizer, device
        )

        #eval
        val_loss, val_similarities, val_labels_epoch, val_dists, val_quartiles, val_projections = evaluate_model(
            projection_net, loss_fn, test_loader, device
        )

        #metrics
        
        #train_acc, train_precision, train_recall, train_f1 = kmeans_metrics(train_projections, train_quartiles, n_clusters=2)
        #val_acc, val_precision, val_recall, val_f1 = kmeans_metrics(val_projections, val_quartiles, n_clusters=2)

        val_acc, val_precision, val_recall, val_f1 = logreg_metrics(train_projections, train_quartiles, val_projections, val_quartiles)

        train_losses.append(train_loss)
        val_losses.append(val_loss)
        #train_accs.append(train_acc)
        val_accs.append(val_acc)

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            best_model_state = projection_net.state_dict().copy()
            tqdm.write(f" Epoch {epoch+1}: New best val loss: {best_val_loss:.4f}")
        else:
            patience_counter += 1

        if (epoch + 1) % 10 == 0:
            tqdm.write(f"Epoch {epoch+1}/{NUM_EPOCHS}")
            tqdm.write(f"  Train Loss: {train_loss:.4f}") #, Train Acc: {train_acc:.4f}")
            tqdm.write(f"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")
            tqdm.write(f"  Patience: {patience_counter}/{PATIENCE}")
            if USE_LEARNABLE:
                tqdm.write(f"  Alpha: {loss_fn.alpha.item():.4f}, Beta: {loss_fn.beta.item():.4f}")

        if (epoch + 1) % 20 == 0:
            tqdm.write("\n=== Distance Clustering Analysis ===")
            similar_mean, dissimilar_mean, separation = plot_distance_clustering(
                train_dists, train_labels_epoch, f"Training - Epoch {epoch+1}", final=False
            )
            tqdm.write(f"Separation: {separation:.4f}\n")

        if patience_counter >= PATIENCE:
            tqdm.write(f"\n Early stopping at epoch {epoch+1}")
            break

    if best_model_state is not None:
        print("\n Restoring best model...")
        projection_net.load_state_dict(best_model_state)

    print("\n" + "="*80)
    print("TRAINING COMPLETE")
    print("="*80 + "\n")

    plot_training_metrics(train_losses, val_losses, train_accs, val_accs)

    print("\n=== FINAL EVALUATION ===")
    final_loss, final_similarities, final_labels, final_dists, final_quartiles, final_projections = evaluate_model(
        projection_net, loss_fn, test_loader, device, n_clusters=2
    )

    test_loss = final_loss
    #test_acc, test_precision, test_recall, test_f1 = kmeans_metrics(final_projections, final_quartiles, n_clusters=2)
    test_acc, test_precision, test_recall, test_f1 = logreg_metrics(train_projections, train_quartiles, final_projections, final_quartiles)
    test_auc = roc_auc_score(final_labels, final_similarities) #auc based on similarities

    print(f"\nTest Metrics:")
    print(f"  Loss: {test_loss:.4f}")
    print(f"  Accuracy: {test_acc:.4f}")
    print(f"  Precision: {test_precision:.4f}")
    print(f"  Recall: {test_recall:.4f}")
    print(f"  F1: {test_f1:.4f}")
    print(f"  AUC: {test_auc:.4f}")
    
    original_acc, original_precision, original_recall, original_f1 = logreg_metrics(train_dataset.embeddings, train_dataset.quartiles, test_dataset.embeddings, test_dataset.quartiles)
    
    print(f"\nOriginal Dataset Metrics:")
    print(f"  Accuracy: {original_acc:.4f}")
    print(f"  Precision: {original_precision:.4f}")
    print(f"  Recall: {original_recall:.4f}")
    print(f"  F1: {original_f1:.4f}")

    with open(RESULTS_FILE, 'a') as f:
        f.write(f'{RUN_NAME},{test_loss},{test_acc},{test_precision},{test_recall},{test_f1},{test_auc},{original_acc},{original_f1}\n')

    print(f"\nDistance Statistics:")
    print(f"  Mean: {np.mean(final_dists):.4f}")
    print(f"  Std: {np.std(final_dists):.4f}")
    print(f"  Min: {np.min(final_dists):.4f}")
    print(f"  Max: {np.max(final_dists):.4f}")

    print("\n=== Final Distance Clustering ===")
    plot_distance_clustering(final_dists, final_labels, "Final Test Set", final=True)

    print("\n=== Embedding Visualization ===")
    sample_size = min(500, len(test_seqs))
    sample_indices = np.random.choice(len(test_seqs), sample_size, replace=False)

    sample_seqs = [test_seqs[i] for i in sample_indices]
    sample_dms = [test_dms[i] for i in sample_indices]
    sample_embeddings = torch.stack([seq_to_embedding[seq] for seq in sample_seqs])

    visualize_embeddings_tsne(sample_embeddings, sample_dms,
                              "Original ESM Layer 11 Embeddings (DMS Colored)")

    with torch.no_grad():
        projected_embeddings = projection_net(sample_embeddings.to(device))

    visualize_embeddings_tsne(projected_embeddings, sample_dms,
                              "Projected Embeddings (DMS Colored)")

    print("\n=== Saving Model ===")
    torch.save({
        'model_state_dict': projection_net.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'train_losses': train_losses,
        'val_losses': val_losses,
        'config': {
            'distance_metric': DISTANCE_METRIC,
            'hidden_dims': HIDDEN_DIMS,
            'learning_rate': LEARNING_RATE,
            'best_val_loss': best_val_loss
        }
    }, f'{RESULTS_DIR}/{RUN_NAME}.pt')
    print(f" Model saved to '{RESULTS_DIR}/{RUN_NAME}.pt'")

main()

print('Pipeline completed successfully')

"""
import pandas as pd

# Load the data
df = pd.read_csv(DATA_PATH)

print("DMS Score Statistics:")
print(df['DMS_score'].describe())
print(f"\nMin: {df['DMS_score'].min()}")
print(f"Max: {df['DMS_score'].max()}")
print(f"Range: {df['DMS_score'].max() - df['DMS_score'].min()}")

# Check for outliers
print(f"\nValues > 10: {(df['DMS_score'] > 10).sum()}")
print(f"Values > 20: {(df['DMS_score'] > 20).sum()}")
print(f"Values > 40: {(df['DMS_score'] > 40).sum()}")

# Show some high values
print("\nTop 10 highest DMS scores:")
print(df.nlargest(10, 'DMS_score')[['DMS_score', 'filename', 'mutant']])

# Check which genes have extreme DMS scores
extreme_threshold = 10

extreme_rows = df[df['DMS_score'] > extreme_threshold]
print(f"Rows with DMS_score > {extreme_threshold}: {len(extreme_rows)}")
print("\nGenes with extreme values:")
print(extreme_rows.groupby('filename')['DMS_score'].agg(['count', 'min', 'max', 'mean']))

# Show the actual extreme values
print("\nTop 20 highest DMS scores:")
print(df.nlargest(20, 'DMS_score')[['DMS_score', 'filename', 'mutant', 'mutated_sequence']])

"""
