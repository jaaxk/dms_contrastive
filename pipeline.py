# -*- coding: utf-8 -*-
"""dms_cosine_jv.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PJEilEw4yQiz4Y8PgGoiay8RDkPvwIsQ
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.linear_model import LogisticRegression
import warnings
import sys
import esm
from tqdm import tqdm
import pickle
import os
import argparse
from transformers import AutoTokenizer, AutoModel
import scripts.h5_utils as h5_utils
import json
import re
from contextlib import redirect_stdout, redirect_stderr

warnings.filterwarnings('ignore')

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

def parse_args():
    parser = argparse.ArgumentParser(
        prog='DMS_contrastive',
        description='DMS contrastive learning pipeline'
    )

    parser.add_argument('--run_name', type=str, required=True)

    #ML hyperparams
    parser.add_argument('--batch_size', type=int, default=32)
    parser.add_argument('--learning_rate', type=float, default=0.0001)
    parser.add_argument('--num_epochs', type=int, default=200)
    parser.add_argument('--patience', type=int, default=50)
    parser.add_argument('--distance_metric', choices=['cosine', 'euclidean'], default='cosine')
    parser.add_argument('--use_learnable', type=bool, default=False)

    #model hyperparams
    parser.add_argument('--hidden_dims', type=list, default=[512, 256, 128])
    parser.add_argument('--normalize_output', type=bool, default=True)
    parser.add_argument('--freeze_esm', action='store_true', default=False)
    parser.add_argument('--esm_layer', type=int, default=33)
    parser.add_argument('--model_name', default='facebook/esm2_t33_650M_UR50D')

    #data
    parser.add_argument('--embeddings_path', default=None)
    parser.add_argument('--data_path', default='dms_data/datasets/Stability.csv')
    parser.add_argument('--split_by_gene', choices=[True, False], default=True, help='train/test split by gene rather than variant')
    parser.add_argument('--base_results_dir', default='results')
    parser.add_argument('--same_gene_batch', action='store_true', help='each batch contains a single gene, no cross-gene pairs', default=False)
    parser.add_argument('--model_cache', default=None)
    parser.add_argument('--esm_max_length', type=int, default=75)
    parser.add_argument('--input_dim', type=int, default=1280)
    parser.add_argument('--dropout', type=float, default=0.1)
    parser.add_argument('--metadata_path', default=None)
    parser.add_argument('--normalize_to_wt', action='store_true', default=False)
    parser.add_argument('--ohe_baseline', action='store_true', default=False)

    args = parser.parse_args()
    return args

args = parse_args()

#EMBEDDINGS_PATH = args.embeddings_path
DATA_PATH = args.data_path
RUN_NAME = args.run_name

#params
BATCH_SIZE = args.batch_size
LEARNING_RATE = args.learning_rate
NUM_EPOCHS = args.num_epochs
PATIENCE = args.patience
DISTANCE_METRIC = args.distance_metric
USE_LEARNABLE = args.use_learnable

# Model architecture
HIDDEN_DIMS = args.hidden_dims
NORMALIZE_OUTPUT = args.normalize_output

if args.model_cache is not None:
    torch.hub.set_dir(args.model_cache)
#results dir
RESULTS_DIR = f'{args.base_results_dir}/{args.run_name}'
os.makedirs(RESULTS_DIR, exist_ok=True)
json.dump(vars(args), open(f'{RESULTS_DIR}/args.json', 'w'))
os.makedirs(f'{RESULTS_DIR}/training_figs', exist_ok=True)
RESULTS_FILE = f'{args.base_results_dir}/results.csv'
if not os.path.exists(RESULTS_FILE):
    with open(RESULTS_FILE, 'w') as f:
        f.write('run_name,test_loss,test_acc,test_precision,test_recall,test_f1,test_auc\n')

if args.normalize_to_wt:
    if not args.metadata_path:
        raise ValueError("--metadata_path must be specified if --normalize_to_wt is set")
    

#init ESM model
esm_model = AutoModel.from_pretrained(args.model_name)
tokenizer = AutoTokenizer.from_pretrained(args.model_name)

esm_model.eval() #REMOVE once we are finetuning
esm_model.to(device)

if args.ohe_baseline:
    repo_path = os.path.abspath("/gpfs/scratch/jvaska/brandes_lab/esm-variants")
    if not os.path.exists(repo_path):
        raise ValueError(f"ESM-variants repository not found at {repo_path}")
    sys.path.append(repo_path)
    import esm_variants_utils

    #load model
    esm_model_llr, esm_alphabet_llr = esm.pretrained.esm2_t33_650M_UR50D()
    esm_batch_converter_llr = esm_alphabet_llr.get_batch_converter()

# Set all random seeds for reproducibility
def set_seed(seed=42):
    import random
    import numpy as np
    import torch
    import os
    
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)

set_seed(42)  # Call this at the start of your script

def esm_batch(sequences):
    #rint('ESM BATCH METHOD')
    tokenized = tokenizer(sequences, padding=True, truncation=True, max_length=args.esm_max_length, return_tensors="pt")
    batch_tokens = tokenized['input_ids'].to(device)
    attention_mask = tokenized['attention_mask'].to(device)
    with torch.no_grad(): #remove this once we're ready to finetune
        results = esm_model(input_ids=batch_tokens, attention_mask=attention_mask, output_hidden_states=False)
    reps = results["last_hidden_state"].to(device)    
    #mean pooling
    mask = attention_mask.unsqueeze(-1)          # (B, L, 1)
    masked_reps = reps * mask
    sum_reps = masked_reps.sum(dim=1)
    lengths = mask.sum(dim=1)
    embeddings_batch = sum_reps / lengths         # (B, H)
    out = embeddings_batch

    return out

class DMSContrastiveDataset(Dataset):
    def __init__(self, sequences, quartiles, dms_scores, genes, mutants=None):
        self.sequences = sequences
        self.quartiles = quartiles
        self.dms_scores = dms_scores
        #self.seq_to_embedding = seq_to_embedding
        self.genes = genes
        self.mutants = mutants

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        sequence = self.sequences[idx]
        quartile = self.quartiles[idx]
        dms_score = self.dms_scores[idx]
        gene = self.genes[idx]
        mutant = self.mutants[idx] if self.mutants is not None else None

        #get precomputed embedding
        #emb = self.seq_to_embedding[seq]

        return {
            'quartile': quartile,
            'dms_score': dms_score,
            'sequence': sequence,
            'gene': gene,
            'mutant': mutant
        }

def load_embeddings_h5(sequences):
    if args.embeddings_path is not None:
        embeddings, missing_seqs, missing_indices = h5_utils.load_embeddings(args.embeddings_path, sequences)
        
        if len(missing_seqs) > 0:
            missing_embeddings = esm_batch(missing_seqs)
            h5_utils.save_embeddings(args.embeddings_path, missing_seqs, missing_embeddings)
            #insert missing embeddings back into embeddings
            for i, idx in enumerate(missing_indices):
                embeddings[idx] = missing_embeddings[i]
    else: 
        raise NotImplementedError('embeddings_path must be specified')

    embeddings = torch.stack([
        torch.as_tensor(e, dtype=torch.float32).to(device)
        for e in embeddings
    ])

    return embeddings

class DataLoader():
    def __init__(self, dataset, batch_size=16, shuffle=True, balance_quartiles=True, gene_to_wt=None, gene_aware=False):
        self.dataset = dataset
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.balance_quartiles = balance_quartiles
        self.gene_to_wt = gene_to_wt
        self.gene_aware = gene_aware

        if gene_aware:
            self.gene_groups = {}
            for i, gene in enumerate(dataset.genes):
                if gene not in self.gene_groups:
                    self.gene_groups[gene] = {'high': [], 'low': []}
                quartile = dataset.quartiles[i]
                self.gene_groups[gene][quartile].append(i)

            print(f"Gene-aware loader created for {len(self.gene_groups)} genes")
            for gene, quartile_dict in list(self.gene_groups.items())[:3]:
                print(f"  {gene}: {len(quartile_dict['high'])} high, {len(quartile_dict['low'])} low")
        else:
            print('Using basic data loader - no gene grouping')
        
        # Standard behavior - no gene grouping
        self.indices = list(range(len(dataset)))
        if self.shuffle:
            np.random.shuffle(self.indices)

    def __iter__(self):
        # Separate indices by quartile
        if self.gene_aware:
            all_batches = []

            for gene, quartile_dict in self.gene_groups.items():
                high_indices = quartile_dict['high'].copy()
                low_indices = quartile_dict['low'].copy()

                if self.shuffle:
                    np.random.shuffle(high_indices)
                    np.random.shuffle(low_indices)

                if self.balance_quartiles and len(high_indices) > 0 and len(low_indices) > 0:
                    half_batch = self.batch_size // 2

                    num_batches = min(len(high_indices) // half_batch, len(low_indices) // half_batch)

                    for i in range(num_batches):
                        batch_high = high_indices[i*half_batch:(i+1)*half_batch]
                        batch_low = low_indices[i*half_batch:(i+1)*half_batch]
                        batch_indices = batch_high + batch_low

                        if self.shuffle:
                            np.random.shuffle(batch_indices)

                        batch = [self.dataset[idx] for idx in batch_indices]
                        all_batches.append(batch)

            if self.shuffle:
                np.random.shuffle(all_batches)

            for batch in all_batches:
                yield self._collate_batch(batch)

        else:

            high_indices = [i for i, q in enumerate(self.dataset.quartiles) if q == 'high']
            low_indices = [i for i, q in enumerate(self.dataset.quartiles) if q == 'low']
            
            if self.shuffle:
                np.random.shuffle(high_indices)
                np.random.shuffle(low_indices)
            
            # Determine the number of batches we can make
            half_batch = self.batch_size // 2
            num_batches = min(len(high_indices), len(low_indices)) // half_batch
            
            for i in range(num_batches):
                # Take equal number of high and low quartile samples
                batch_high = high_indices[i*half_batch:(i+1)*half_batch]
                batch_low = low_indices[i*half_batch:(i+1)*half_batch]
                batch_indices = batch_high + batch_low
                
                # Shuffle the combined batch
                if self.shuffle:
                    np.random.shuffle(batch_indices)
                    
                batch = [self.dataset[idx] for idx in batch_indices]
                yield self._collate_batch(batch)

    def _collate_batch(self, batch):
        #embeddings = torch.stack([item['embedding'] for item in batch])
        quartiles = [item['quartile'] for item in batch]
        dms_scores = [item['dms_score'] for item in batch]
        sequences = [item['sequence'] for item in batch]
        genes = [item['gene'] for item in batch]

        if args.normalize_to_wt:
            wt_sequences = [self.gene_to_wt[gene] for gene in genes]
            wt_embeddings = load_embeddings_h5(wt_sequences)
        else:
            wt_embeddings = None

        if args.ohe_baseline and self.gene_aware:
            #print('gene-aware ohe batch iteration')
            ohe_features = get_ohe_features(wt_sequences, [item['mutant'] for item in batch])
        else:
            ohe_features = None

        embeddings = load_embeddings_h5(sequences)

        return {
            'quartiles': quartiles,
            'dms_scores': dms_scores,
            'sequences': sequences,
            'genes': genes,
            'wt_embeddings': wt_embeddings,
            'embeddings': embeddings,
            'ohe_features': ohe_features
        }

    def __len__(self):
        return len(self.indices) // self.batch_size

class ContrastiveNetwork(nn.Module):
    def __init__(self, esm_model, input_dim=1280, hidden_dims=[512, 256, 128], normalize_output=False, esm_layer=33, esm_only=False, normalize_to_wt=False):
        super(ContrastiveNetwork, self).__init__()

        if esm_layer != 33:
            raise ValueError('change forward method to output all hidden states')

        self.esm = esm_model
        self.esm_layer = esm_layer
        self.esm_only = esm_only
        self.normalize_to_wt = normalize_to_wt

        layers = []
        prev_dim = input_dim

        layers.append(nn.Dropout(args.dropout))

        for i, hidden_dim in enumerate(hidden_dims):
            layers.append(nn.Linear(prev_dim, hidden_dim))

            if i < len(hidden_dims) - 1:
                layers.append(nn.LayerNorm(hidden_dim))
                layers.append(nn.ReLU())

            prev_dim = hidden_dim

        self.projection = nn.Sequential(*layers)
        self.normalize_output = normalize_output

    def forward(self, batch):

        out = batch['embeddings']

        if self.normalize_to_wt:
            out = out - batch['wt_embeddings']

        if self.esm_only:
            x = out
        else:
            x = self.projection(out) 
        
        if self.normalize_output:
            #l2 normalize
            x = nn.functional.normalize(x, p=2, dim=-1)
        return x

        """#print('forward 1'
        out = batch['embeddings']
        #reps = out["representations"][self.esm_layer]
        #embs = reps.mean(dim=1) #mean pool
        x = self.projection(out) 
        if self.normalize_output:
            #l2 normalize
            x = nn.functional.normalize(x, p=2, dim=-1)
        return x"""

class ContrastiveLoss(nn.Module):
    def __init__(self, distance_metric="cosine", use_learnable=True):
        super(ContrastiveLoss, self).__init__()
        self.distance_metric = distance_metric
        self.use_learnable = use_learnable

        if use_learnable:
            if distance_metric == "euclidean":
                self.alpha = nn.Parameter(torch.tensor(-10.0))
            else:  #cosine
                #positive: high similarity → high probability
                self.alpha = nn.Parameter(torch.tensor(10.0))
            self.beta = nn.Parameter(torch.tensor(0.0))

    def forward(self, embeddings, quartiles):
        batch_size = embeddings.shape[0]
        device = embeddings.device

        if self.distance_metric == "euclidean":
            norms = (embeddings ** 2).sum(dim=1, keepdim=True)  # (B, 1)
            distance_matrix = norms + norms.t() - 2 * torch.mm(embeddings, embeddings.t())
            distance_matrix = torch.sqrt(torch.clamp(distance_matrix, min=1e-8))  # (B, B)

            if self.use_learnable:
                logit_matrix = self.alpha * distance_matrix + self.beta
                similarity_matrix = torch.sigmoid(logit_matrix)
            else:
                similarity_matrix = torch.exp(-distance_matrix)
                similarity_matrix = torch.clamp(similarity_matrix, min=1e-7, max=1-1e-7)

        elif self.distance_metric == "cosine":
            embeddings_norm = nn.functional.normalize(embeddings, p=2, dim=1)  # (B, D)

            #pairwise cosine similarity: normalized dot products
            cosine_sim_matrix = torch.mm(embeddings_norm, embeddings_norm.t())  # (B, B)

            #distance = 1 - cosine_sim
            distance_matrix = 1 - cosine_sim_matrix

            if self.use_learnable:
                logit_matrix = self.alpha * cosine_sim_matrix + self.beta
                similarity_matrix = torch.sigmoid(logit_matrix)
            else:
                # Fixed transformation: map [-1, 1] to [0, 1]
                similarity_matrix = (cosine_sim_matrix * 0.5) + 0.5
                similarity_matrix = torch.clamp(similarity_matrix, min=1e-7, max=1-1e-7)
        else:
            raise ValueError(f"Unknown distance metric: {self.distance_metric}")

        #create label matrix: 1 if same quartile, 0 if different
        quartile_tensor = torch.tensor([1 if q == 'high' else 0 for q in quartiles],
                                       dtype=torch.long, device=device)
        label_matrix = (quartile_tensor.unsqueeze(0) == quartile_tensor.unsqueeze(1)).float()

        #extract upper triangle (excluding diagonal) for pairs
        upper_triangle_mask = torch.triu(torch.ones_like(label_matrix), diagonal=1).bool()

        similarities = similarity_matrix[upper_triangle_mask]
        labels = label_matrix[upper_triangle_mask]
        distances = distance_matrix[upper_triangle_mask]

        #compute loss
        loss = nn.BCELoss()(similarities, labels)

        all_similarities = similarities.detach().cpu().tolist()
        all_distances = distances.detach().cpu().tolist()
        all_labels = labels.cpu().tolist()

        return loss, all_similarities, all_distances, all_labels

def load_and_preprocess_data(data_path):
    print(f"Loading data from {data_path}")
    df = pd.read_csv(data_path)

    df = df.dropna(subset=['DMS_score', 'mutated_sequence', 'filename'])

    #get unique genes
    unique_genes = df['filename'].unique()
    print(f"Found {len(unique_genes)} unique genes")

    #process each gene separately (gene-aware quartile calculation)
    all_processed_dfs = []

    for gene in unique_genes:
        gene_df = df[df['filename'] == gene].copy()

        if len(gene_df) < 10:
            print(f"Skipping gene {gene} (only {len(gene_df)} samples)")
            continue

        #calculate quartiles for THIS gene
        q25 = gene_df['DMS_score'].quantile(0.25)
        q75 = gene_df['DMS_score'].quantile(0.75)

        #filter to top 75% and bottom 25% for this gene
        top_75 = gene_df[gene_df['DMS_score'] >= q75].copy()
        bottom_25 = gene_df[gene_df['DMS_score'] <= q25].copy()

        #add quartile labels
        top_75['quartile'] = 'high'
        bottom_25['quartile'] = 'low'

        #combine for this gene
        gene_processed = pd.concat([top_75, bottom_25], ignore_index=True)
        all_processed_dfs.append(gene_processed)

        print(f"  {gene}: {len(gene_processed)} samples (high: {len(top_75)}, low: {len(bottom_25)})")

    #combine all genes
    combined_df = pd.concat(all_processed_dfs, ignore_index=True)
    print(f"\n Total processed samples: {len(combined_df)}")

    return combined_df

def create_train_test_split(df, split_by_gene=True, test_size=0.2):
    print("\nCreating gene-aware train/test split...")

    #get unique sequences
    all_sequences = df['mutated_sequence'].unique()
    print(f"Total unique sequences: {len(all_sequences)}")

    #get unique genes
    all_genes = df['uniprot_id'].unique()
    print(f"Total unique genes: {len(all_genes)}")

    if split_by_gene:
      print('Splitting by gene...')
      train_genes, test_genes = train_test_split(
        all_genes, test_size=test_size, random_state=42
      )

      print(f"Train genes: {len(train_genes)}")
      print(f"Test genes: {len(test_genes)}")

      #create train/test dataframes
      train_df = df[df['uniprot_id'].isin(train_genes)].copy()
      test_df = df[df['uniprot_id'].isin(test_genes)].copy()
      
      train_sequences = train_df['mutated_sequence']
      test_sequences = test_df['mutated_sequence']

    else:
      print('Splitting by sequence...')
      #split sequences
      train_sequences, test_sequences = train_test_split(
          all_sequences, test_size=test_size, random_state=42
      )

      print(f"Train sequences: {len(train_sequences)}")
      print(f"Test sequences: {len(test_sequences)}")

      #create train/test dataframes
      train_df = df[df['mutated_sequence'].isin(train_sequences)].copy()
      test_df = df[df['mutated_sequence'].isin(test_sequences)].copy()

      #get genes
      train_genes = train_df['uniprot_id']
      test_genes = test_df['uniprot_id']

    print(f"Train samples: {len(train_df)}")
    print(f"Test samples: {len(test_df)}")

    #verify no GENE overlap
    overlap = set(train_genes).intersection(set(test_genes))
    if len(overlap) == 0:
        print("✅ No gene overlap between train and test - proper held-out test!")
    else:
        print(f"⚠️  Warning: {len(overlap)} genes overlap")

    #verify no SEQUENCE overlap
    overlap = set(train_sequences).intersection(set(test_sequences))
    if len(overlap) == 0:
        print("✅ No sequence overlap between train and test - proper held-out test!")
    else:
        print(f"⚠️  Warning: {len(overlap)} sequences overlap")

    return train_df, test_df

def get_embeddings_from_model(projection_net, dataloader, device):
    projection_net.eval()
    
    all_projections = []
    
    with torch.no_grad():
        for batch in tqdm(dataloader):
            sequences = batch['sequences'].to(device)
            projected = projection_net(sequences)
            all_projections.extend(projected.cpu().detach().numpy())
    
    return all_projections

def train_epoch(projection_net, loss_fn, dataloader, optimizer, device):
    projection_net.train()

    total_loss = 0
    all_similarities = []
    all_labels = []
    all_distances = []
    all_projections = []
    all_quartiles = []
    num_batches = 0

    for batch in tqdm(dataloader, desc="Training"):
        optimizer.zero_grad()

        #sequences = batch['sequences']
        quartiles = batch['quartiles']

        #project embeddings
        projected = projection_net(batch)
        all_projections.extend(projected.cpu().detach().numpy())

        #compute loss
        loss, similarities, distances, labels = loss_fn(projected, quartiles)

        #backward pass
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        num_batches += 1

        all_similarities.extend(similarities)
        all_distances.extend(distances)
        all_labels.extend(labels)
        all_quartiles.extend(quartiles)

    return total_loss / max(num_batches, 1), all_similarities, all_labels, all_distances, all_quartiles, all_projections

def evaluate_model(projection_net, loss_fn, dataloader, device):
    projection_net.eval()

    all_similarities = []
    all_labels = []
    all_distances = []
    all_kmeans_labels = []
    all_quartiles = []
    all_projections = []
    total_loss = 0
    num_batches = 0

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluation"):
            quartiles = batch['quartiles']

            #project embeddings
            projected = projection_net(batch)
            all_projections.extend(projected.cpu().detach().numpy())

            #compute loss
            loss, similarities, distances, labels = loss_fn(projected, quartiles)

            total_loss += loss.item()
            num_batches += 1

            all_similarities.extend(similarities)
            all_distances.extend(distances)
            all_labels.extend(labels)
            all_quartiles.extend(quartiles)

    avg_loss = total_loss / max(num_batches, 1)
    return avg_loss, all_similarities, all_labels, all_distances, all_quartiles, all_projections

def contrastive_metrics(similarities, labels):
    binary_preds = [1 if sim > 0.5 else 0 for sim in similarities]
    accuracy = accuracy_score(labels, binary_preds)
    precision = precision_score(labels, binary_preds)
    recall = recall_score(labels, binary_preds)
    f1 = f1_score(labels, binary_preds)
    return accuracy, precision, recall, f1

def kmeans_metrics(projections, quartiles, n_clusters=2):
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    kmeans.fit(projections)
    kmeans_labels = kmeans.labels_

    quartiles = [1 if q == 'high' else 0 for q in quartiles]

    accuracy = max(accuracy_score(quartiles, kmeans_labels), 1 - accuracy_score(quartiles, kmeans_labels))
    precision = max(precision_score(quartiles, kmeans_labels), 1 - precision_score(quartiles, kmeans_labels))
    recall = max(recall_score(quartiles, kmeans_labels), 1 - recall_score(quartiles, kmeans_labels))
    f1 = max(f1_score(quartiles, kmeans_labels), 1 - f1_score(quartiles, kmeans_labels))

    return accuracy, precision, recall, f1

def logreg_metrics(train_projections, train_quartiles, test_projections, test_quartiles, sample_train=1000):
    # Sample data for efficiency
    if len(train_projections) > sample_train:
        train_indices = np.random.choice(len(train_projections), sample_train, replace=False)
        train_proj_sample = [train_projections[i] for i in train_indices]
        train_quartile_sample = [train_quartiles[i] for i in train_indices]
    else:
        train_proj_sample = train_projections
        train_quartile_sample = train_quartiles
    
    # Convert to numpy arrays
    X_train = np.array(train_proj_sample)
    y_train = np.array([1 if q == 'high' else 0 for q in train_quartile_sample])
    
    X_test = np.array(test_projections)
    y_test = np.array([1 if q == 'high' else 0 for q in test_quartiles])
    
    # Train logistic regression
    clf = LogisticRegression(random_state=42)
    clf.fit(X_train, y_train)
    
    # Predictions
    y_pred = clf.predict(X_test)
    
    # Metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    
    return accuracy, precision, recall, f1

def plot_training_metrics(train_losses, val_losses, train_aucs, val_aucs):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

    #loss plot
    ax1.plot(train_losses, label='Train Loss', color='blue')
    ax1.plot(val_losses, label='Val Loss', color='red')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.set_title('Training and Validation Loss (Cosine Similarity)')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    #accuracy plot
    ax2.plot(train_aucs, label='Train AUC', color='blue')
    ax2.plot(val_aucs, label='Val AUC', color='red')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('AUC')
    ax2.set_title('Training and Validation AUC')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(f'{RESULTS_DIR}/training_metrics.png')

def plot_distance_clustering(distances, labels, title="Distance Clustering", final=False):
    similar_distances = [d for d, l in zip(distances, labels) if l == 1]
    dissimilar_distances = [d for d, l in zip(distances, labels) if l == 0]

    plt.figure(figsize=(12, 5))

    #histogram comparison
    plt.subplot(1, 2, 1)
    plt.hist(similar_distances, bins=30, alpha=0.7, label='Similar (1)', color='blue', density=True)
    plt.hist(dissimilar_distances, bins=30, alpha=0.7, label='Dissimilar (0)', color='red', density=True)
    plt.xlabel('Cosine Distance (1 - cosine_sim)')
    plt.ylabel('Density')
    plt.title(f'{title} - Distribution')
    plt.legend()
    plt.grid(True, alpha=0.3)

    #box plot comparison
    plt.subplot(1, 2, 2)
    data_to_plot = [similar_distances, dissimilar_distances]
    box_plot = plt.boxplot(data_to_plot, labels=['Similar (1)', 'Dissimilar (0)'],
                          patch_artist=True, showfliers=True)
    box_plot['boxes'][0].set_facecolor('lightblue')
    box_plot['boxes'][1].set_facecolor('lightcoral')
    plt.ylabel('Cosine Distance')
    plt.title(f'{title} - Box Plot')
    plt.grid(True, alpha=0.3)

    #add statistics
    similar_mean = np.mean(similar_distances)
    dissimilar_mean = np.mean(dissimilar_distances)
    separation = dissimilar_mean - similar_mean

    plt.figtext(0.5, 0.02,
                f'Similar mean: {similar_mean:.4f} | Dissimilar mean: {dissimilar_mean:.4f} | Separation: {separation:.4f}',
                ha='center', fontsize=10, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray"))

    plt.tight_layout()
    if final:
        plt.savefig(f'{RESULTS_DIR}/{title}.png')
    else:
        plt.savefig(f'{RESULTS_DIR}/training_figs/{title}.png')

    return similar_mean, dissimilar_mean, separation

def visualize_embeddings_tsne(embeddings, labels, title="t-SNE Visualization"):
    print(f"Creating t-SNE visualization: {title}")

    #reduce dimensionality
    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(embeddings)-1))
    embeddings_2d = tsne.fit_transform(embeddings)

    plt.figure(figsize=(10, 8))
    scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1],
                         c=labels, cmap='viridis', alpha=0.7, s=50)
    plt.colorbar(scatter, label='DMS Score')
    plt.title(title)
    plt.xlabel('t-SNE 1')
    plt.ylabel('t-SNE 2')
    plt.tight_layout()
    plt.savefig(f'{RESULTS_DIR}/{title}.png')

def calc_esm_llr_scores(wt_seq):
    _, (raw_llr_scores,) = esm_variants_utils.get_wt_LLR(pd.DataFrame([{'id': '_', 'gene': '_', 'seq': wt_seq, 'length': len(wt_seq)}]), \
            esm_model_llr, esm_alphabet_llr, esm_batch_converter_llr, device="cpu")
    raw_seq_results = raw_llr_scores.transpose().stack().reset_index().rename(columns = {'level_0': 'wt_aa_and_pos', 'level_1': 'mut_aa', \
            0: 'esm_score'})
    return pd.DataFrame({'mut_name': raw_seq_results['wt_aa_and_pos'].str.replace(' ', '') + raw_seq_results['mut_aa'], \
            'esm_score': raw_seq_results['esm_score']}).set_index('mut_name')['esm_score']


def ohe_llr_baseline(loss_fn, dataloader):
    print("Calculating OHE LLR baseline")

    all_losses = []
    all_similarities = []
    all_distances = []
    all_labels = []

    for batch in tqdm(dataloader, desc="OHE LLR Baseline"):
        quartiles = batch['quartiles']
        ohe_features = batch['ohe_features']
        #print(ohe_features)
        #print(ohe_features.shape)
        #print(type(ohe_features))

        #remove nans and stay consistent with quartiles
        nan_idx = np.where(np.isnan(ohe_features).any(axis=1))[0]
        #print(f"Removing {len(nan_idx)} features with nan values")
        ohe_features = np.delete(ohe_features, nan_idx, axis=0)
        quartiles = [quartiles[i] for i in range(len(quartiles)) if i not in nan_idx]
        assert len(ohe_features) == len(quartiles)

        loss, similarities, distances, labels = loss_fn(torch.tensor(ohe_features, dtype=torch.float32), quartiles)
        all_losses.append(loss.item())
        all_similarities.extend(similarities)
        all_distances.extend(distances)
        all_labels.extend(labels)

    auc = roc_auc_score(all_labels, all_similarities)
    acc, precision, recall, f1 = contrastive_metrics(all_similarities, all_labels)

    print(f"OHE LLR Baseline - Loss: {np.mean(all_losses):.4f}, AUC: {auc:.4f}, Acc: {acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}")


    with open(RESULTS_FILE, 'a') as f:
        f.write(f'{RUN_NAME}_OHE+LLR_baseline,{np.mean(all_losses)},{acc},{precision},{recall},{f1},{auc}\n')
        

def get_ohe_features(wt_sequences, mutants):
    #to be used in DataLoader / collate_batch

    #checks
    if len(wt_sequences) != len(mutants):
        raise ValueError('wt_sequences and mutants must have the same length')
    if len(set(wt_sequences)) != 1:
        raise ValueError('wt_sequences must all be the same for GENE AWARE data loader - needs new implementation if we want to change this, but relevant')
    wt_sequence = wt_sequences[0]
    
    #constants
    MISSENSE_PATTERN = re.compile('([A-Z])(\d+)([A-Z])')
    UNIQUE_AAS = list('ACDEFGHIKLMNPQRSTVWY')
    aa_to_index = {aa: i for i, aa in enumerate(UNIQUE_AAS)}

    #make features
    ohe_features = np.zeros((len(mutants), len(wt_sequence), len(UNIQUE_AAS)), dtype = np.int8)

    with open(os.devnull, 'w') as fnull, redirect_stdout(fnull), redirect_stderr(fnull):
        llr_scores = calc_esm_llr_scores(wt_sequence)
    llr_scores_ordered = [llr_scores[mutant] if mutant in llr_scores else np.nan for mutant in mutants]
    llr_scores_reshaped = np.array(llr_scores_ordered).reshape(-1, 1)

    #start with 1s at each WT aa
    for j, aa in enumerate(wt_sequence):
        ohe_features[:, j, aa_to_index[aa]] = 1

    for i, mut_name in enumerate(mutants):
        #print(f'Processing mutation {mut_name}')
        #handle multiple mutations - this isn't really necessary bc llr cant handle multiple, and these will be removed anyway - but lets keep the logic for now
        if ':' in mut_name:
            mut_names = mut_name.split(':')
        else:
            mut_names = [mut_name]

        for mut_name in mut_names:
            (ref_aa, pos, alt_aa), = MISSENSE_PATTERN.findall(mut_name)
            j = int(pos) - 1
            ohe_features[i, j, aa_to_index[ref_aa]] = 0
            ohe_features[i, j, aa_to_index[alt_aa]] = 1 #change wt aa to alt at mutated position

    ohe_features = ohe_features.reshape(len(mutants), -1) #flatten from 3d (n_mutants, seq_len, n_aas) to 2d (n_mutants, seq_len * n_aas)
    ohe_features = np.concatenate([llr_scores_reshaped, ohe_features], axis = 1) #add llr score to beginning
    #print(ohe_features.shape)
    #there will be NaNs if there are two mutated positions (A23Y:A27R), we need to remove these later to stay consistent with quartiles
    return ohe_features



        
        


def main():
    print("="*80)
    print(f"Running Contrastive Learning for {RUN_NAME}")
    print("="*80)
    #print(f"Embeddings Path: {EMBEDDINGS_PATH}")
    print(f"Data Path: {DATA_PATH}")
    print(f"Device: {device}")
    print(f"Distance Metric: {DISTANCE_METRIC}")
    print(f"Learnable Transformation: {USE_LEARNABLE}")
    print(f"Output Normalization: {NORMALIZE_OUTPUT}")
    print("="*80)

    """try:
        from google.colab import drive
        drive.mount('/content/drive')
    except:
        print("Not in Colab environment, skipping drive mount")"""

    #load and preprocess data
    df = load_and_preprocess_data(DATA_PATH)
    print(f'Initial mutated sequence length distribution: {df["mutated_sequence"].str.len().describe()}')
    initial_length = len(df)
    df = df[df['mutated_sequence'].str.len() <= args.esm_max_length]
    filtered_count = initial_length - len(df)
    if filtered_count > 0:
        print(f"Filtered {filtered_count} sequences with length > {args.esm_max_length}")
    #df = df.sample(100, random_state=42) #for testing

    if args.normalize_to_wt:
        gene_to_wt = pd.read_csv(args.metadata_path)
        gene_to_wt = gene_to_wt.set_index('DMS_filename')['target_seq'].to_dict()
    else:
        gene_to_wt = None

    #create train/test split
    train_df, test_df = create_train_test_split(df, split_by_gene=args.split_by_gene, test_size=0.2)

    #extract data
    train_seqs = train_df['mutated_sequence'].tolist()
    train_quarts = train_df['quartile'].tolist()
    train_dms = train_df['DMS_score'].tolist()
    train_genes = train_df['filename'].tolist()

    test_seqs = test_df['mutated_sequence'].tolist()
    test_quarts = test_df['quartile'].tolist()
    test_dms = test_df['DMS_score'].tolist()
    test_genes = test_df['filename'].tolist()
    if args.ohe_baseline:
        test_mutants = test_df['mutant'].tolist()
    else:
        test_mutants = None

    print(f'train seq length distribution: {pd.Series(train_seqs).str.len().describe()}')
    print(f'test seq length distribution: {pd.Series(test_seqs).str.len().describe()}')

    #init h5 file for embeddings
    if args.embeddings_path is not None:
        h5_utils.init_h5(args.embeddings_path, len(train_seqs)+len(test_seqs)+filtered_count, embed_dim=args.input_dim)
    else:
        raise ValueError("Embeddings path must be specified")
    

    #create datasets
    train_dataset = DMSContrastiveDataset(train_seqs, train_quarts, train_dms,
                                          train_genes)
    test_dataset = DMSContrastiveDataset(test_seqs, test_quarts, test_dms,
                                         test_genes, test_mutants)

    #create data loaders
    if args.same_gene_batch:
        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, gene_to_wt = gene_to_wt, shuffle=True, gene_aware=True)
        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, gene_to_wt = gene_to_wt, shuffle=False, gene_aware=True)
    else:
        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, gene_to_wt = gene_to_wt, shuffle=True, gene_aware=False)
        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, gene_to_wt = gene_to_wt, shuffle=False, gene_aware=False)
        gene_aware_test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, gene_to_wt=gene_to_wt, shuffle=False, gene_aware=True)


    print(f"\n Data loaders created")
    print(f"   Train batches: ~{len(train_loader)}")
    print(f"   Test batches: ~{len(test_loader)}")

    #check first batch
    first_batch = next(iter(train_loader))
    quartiles_in_batch = first_batch['quartiles']
    genes_in_batch = first_batch['genes']
    high_count = sum(1 for q in quartiles_in_batch if q == 'high')
    low_count = sum(1 for q in quartiles_in_batch if q == 'low')
    unique_genes = len(set(genes_in_batch))
    print(f"\n First batch check:")
    print(f"   Batch size: {len(quartiles_in_batch)}")
    print(f"   High: {high_count}, Low: {low_count}")
    print(f"   Unique genes in batch: {unique_genes} (should be 1 for gene-aware)")

    #init model
    projection_net = ContrastiveNetwork(
        esm_model=esm_model,
        input_dim=args.input_dim,
        hidden_dims=HIDDEN_DIMS,
        normalize_output=NORMALIZE_OUTPUT,
        esm_layer=args.esm_layer,
        normalize_to_wt=args.normalize_to_wt
    ).to(device)

    loss_fn = ContrastiveLoss(
        distance_metric=DISTANCE_METRIC,
        use_learnable=USE_LEARNABLE
    )

    optimizer = optim.Adam(projection_net.parameters(), lr=LEARNING_RATE)

    print(f"\n Model initialized")
    print(f"   Parameters: {sum(p.numel() for p in projection_net.parameters()):,}")
    if USE_LEARNABLE:
        print(f"   Learnable alpha: {loss_fn.alpha.item():.4f}")
        print(f"   Learnable beta: {loss_fn.beta.item():.4f}")


    if args.ohe_baseline:
        ohe_llr_baseline(loss_fn, gene_aware_test_loader)

    #train loop
    train_losses = []
    val_losses = []
    train_accs = []
    val_accs = []
    train_aucs = []
    val_aucs = []

    best_val_loss = float('inf')
    best_val_auc = float('-inf')
    patience_counter = 0
    best_model_state = None
    
    print(f"\n{'='*80}")
    print("STARTING TRAINING")
    print(f"{'='*80}\n")

    for epoch in range(NUM_EPOCHS):
        print(f"\nEpoch {epoch+1}/{NUM_EPOCHS}")
        #train
        train_loss, train_similarities, train_labels_epoch, train_dists, train_quartiles, train_projections = train_epoch(
            projection_net, loss_fn, train_loader, optimizer, device
        )

        #eval
        val_loss, val_similarities, val_labels_epoch, val_dists, val_quartiles, val_projections = evaluate_model(
            projection_net, loss_fn, test_loader, device
        )

        #metrics
        
        #train_acc, train_precision, train_recall, train_f1 = kmeans_metrics(train_projections, train_quartiles, n_clusters=2)
        #val_acc, val_precision, val_recall, val_f1 = kmeans_metrics(val_projections, val_quartiles, n_clusters=2)
        #val_acc, val_precision, val_recall, val_f1 = logreg_metrics(train_projections, train_quartiles, val_projections, val_quartiles)

        val_acc, val_precision, val_recall, val_f1 = contrastive_metrics(val_similarities, val_labels_epoch)
        train_acc, train_precision, train_recall, train_f1 = contrastive_metrics(train_similarities, train_labels_epoch)
        train_auc = roc_auc_score(train_labels_epoch, train_similarities)
        val_auc = roc_auc_score(val_labels_epoch, val_similarities)

        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accs.append(train_acc)
        val_accs.append(val_acc)
        train_aucs.append(train_auc)
        val_aucs.append(val_auc)

        if val_auc > best_val_auc:
            best_val_auc = val_auc
            patience_counter = 0
            best_model_state = projection_net.state_dict().copy()
            print(f" Epoch {epoch+1}: New best val auc: {best_val_auc:.4f}")
        else:
            patience_counter += 1

        #if (epoch + 1) % 5 == 0:
        print(f"Epoch {epoch+1}/{NUM_EPOCHS}")
        print(f"  Train Loss: {train_loss:.4f}") #, Train Acc: {train_acc:.4f}")
        print(f"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")
        print(f"  Patience: {patience_counter}/{PATIENCE}")
        print(f"  Train AUC: {train_auc:.4f}, Val AUC: {val_auc:.4f}")
        if USE_LEARNABLE:
            print(f"  Alpha: {loss_fn.alpha.item():.4f}, Beta: {loss_fn.beta.item():.4f}")

        if (epoch + 1) % 20 == 0:
            print("\n=== Distance Clustering Analysis ===")
            similar_mean, dissimilar_mean, separation = plot_distance_clustering(
                train_dists, train_labels_epoch, f"Training - Epoch {epoch+1}", final=False
            )
            print(f"Separation: {separation:.4f}\n")

        if patience_counter >= PATIENCE:
            print(f"\n Early stopping at epoch {epoch+1}")
            break

    if best_model_state is not None:
        print("\n Restoring best model...")
        projection_net.load_state_dict(best_model_state)

    print("\n" + "="*80)
    print("TRAINING COMPLETE")
    print("="*80 + "\n")

    plot_training_metrics(train_losses, val_losses, train_aucs, val_aucs)

    print("\n=== FINAL EVALUATION ===")

    #different-gene batch evaluation
    print('main batch evaluation')
    final_loss, final_similarities, final_labels, final_dists, final_quartiles, final_projections = evaluate_model(
        projection_net, loss_fn, test_loader, device
    )
    test_loss = final_loss
    test_acc, test_precision, test_recall, test_f1 = contrastive_metrics(final_similarities, final_labels)
    test_auc = roc_auc_score(final_labels, final_similarities) #auc based on similarities

    #Gene-aware evaluation
    print('gene-aware batch evaluation')
    gene_aware_final_loss, gene_aware_final_similarities, gene_aware_final_labels, gene_aware_final_dists, gene_aware_final_quartiles, gene_aware_final_projections = evaluate_model(
        projection_net, loss_fn, gene_aware_test_loader, device
    )
    gene_aware_test_acc, gene_aware_test_precision, gene_aware_test_recall, gene_aware_test_f1 = contrastive_metrics(gene_aware_final_similarities, gene_aware_final_labels)
    gene_aware_test_auc = roc_auc_score(gene_aware_final_labels, gene_aware_final_similarities)

    #original ESM evaluation - normalize to wt
    if args.normalize_to_wt:
        esm_only_model_normalize = ContrastiveNetwork(
            esm_model=esm_model,
            input_dim=args.input_dim,
            hidden_dims=HIDDEN_DIMS,
            normalize_output=NORMALIZE_OUTPUT,
            esm_layer=args.esm_layer,
            esm_only=True,
            normalize_to_wt=True
        ).to(device)
        print('og esm embeddings evaluation - main batch type - normalize to wt')
        original_normalize_test_loss, original_normalize_test_similarities, original_normalize_test_labels, original_normalize_test_dists, original_normalize_test_quartiles, original_normalize_test_projections = evaluate_model(
            esm_only_model_normalize, loss_fn, test_loader, device
        )
        original_normalize_test_acc, original_normalize_test_precision, original_normalize_test_recall, original_normalize_test_f1 = contrastive_metrics(original_normalize_test_similarities, original_normalize_test_labels)
        original_normalize_test_auc = roc_auc_score(original_normalize_test_labels, original_normalize_test_similarities)

        print('og esm embeddings evaluation - gene-aware batch type - normalize to wt')
        original_normalize_gene_aware_test_loss, original_normalize_gene_aware_test_similarities, original_normalize_gene_aware_test_labels, original_normalize_gene_aware_test_dists, original_normalize_gene_aware_test_quartiles, original_normalize_gene_aware_test_projections = evaluate_model(
            esm_only_model_normalize, loss_fn, gene_aware_test_loader, device
        )
        original_normalize_gene_aware_test_acc, original_normalize_gene_aware_test_precision, original_normalize_gene_aware_test_recall, original_normalize_gene_aware_test_f1 = contrastive_metrics(original_normalize_gene_aware_test_similarities, original_normalize_gene_aware_test_labels)
        original_normalize_gene_aware_test_auc = roc_auc_score(original_normalize_gene_aware_test_labels, original_normalize_gene_aware_test_similarities)
    else:
        original_normalize_test_loss = '-'
        original_normalize_test_similarities = '-'
        original_normalize_test_labels = '-'
        original_normalize_test_dists = '-'
        original_normalize_test_quartiles = '-'
        original_normalize_test_projections = '-'
        original_normalize_test_acc = '-'
        original_normalize_test_precision = '-'
        original_normalize_test_recall = '-'
        original_normalize_test_f1 = '-'
        original_normalize_test_auc = '-'


    #otiginal ESM eval - NOT normalizing to wt
    esm_only_model_no_normalize = ContrastiveNetwork(
        esm_model=esm_model,
        input_dim=args.input_dim,
        hidden_dims=HIDDEN_DIMS,
        normalize_output=NORMALIZE_OUTPUT,
        esm_layer=args.esm_layer,
        esm_only=True,
        normalize_to_wt=False
    ).to(device)
    print('og esm embeddings evaluation - main batch type - not normalize to wt')
    original_test_loss, original_test_similarities, original_test_labels, original_test_dists, original_test_quartiles, original_test_projections = evaluate_model(
        esm_only_model_no_normalize, loss_fn, test_loader, device
    )
    original_test_acc, original_test_precision, original_test_recall, original_test_f1 = contrastive_metrics(original_test_similarities, original_test_labels)
    original_test_auc = roc_auc_score(original_test_labels, original_test_similarities)

    print('og esm embeddings evaluation - gene-aware batch type - not normalize to wt')
    original_gene_aware_test_loss, original_gene_aware_test_similarities, original_gene_aware_test_labels, original_gene_aware_test_dists, original_gene_aware_test_quartiles, original_gene_aware_test_projections = evaluate_model(
        esm_only_model_no_normalize, loss_fn, gene_aware_test_loader, device
    )
    original_gene_aware_test_acc, original_gene_aware_test_precision, original_gene_aware_test_recall, original_gene_aware_test_f1 = contrastive_metrics(original_gene_aware_test_similarities, original_gene_aware_test_labels)
    original_gene_aware_test_auc = roc_auc_score(original_gene_aware_test_labels, original_gene_aware_test_similarities)


    print(f"\nTest Metrics:")
    print(f"  Loss: {test_loss:.4f}")
    print(f"  Accuracy: {test_acc:.4f}")
    print(f"  Precision: {test_precision:.4f}")
    print(f"  Recall: {test_recall:.4f}")
    print(f"  F1: {test_f1:.4f}")
    print(f"  AUC: {test_auc:.4f}")
    
    #original_acc, original_precision, original_recall, original_f1 = logreg_metrics(train_dataset.embeddings, train_dataset.quartiles, test_dataset.embeddings, test_dataset.quartiles)
    
    #print(f"\nOriginal Dataset Metrics:")
    #print(f"  Accuracy: {original_acc:.4f}")
    #print(f"  Precision: {original_precision:.4f}")
    #print(f"  Recall: {original_recall:.4f}")
    #print(f"  F1: {original_f1:.4f}")

    with open(RESULTS_FILE, 'a') as f:
        f.write(f'{RUN_NAME},{test_loss},{test_acc},{test_precision},{test_recall},{test_f1},{test_auc}\n')
        f.write(f'{RUN_NAME}_gene_aware_eval,-,{gene_aware_test_acc},{gene_aware_test_precision},{gene_aware_test_recall},{gene_aware_test_f1},{gene_aware_test_auc}\n')
        f.write(f'{RUN_NAME}_og_esm_normalize,-,{original_normalize_test_acc},{original_normalize_test_precision},{original_normalize_test_recall},{original_normalize_test_f1},{original_normalize_test_auc}\n')
        f.write(f'{RUN_NAME}_og_esm_normalize_gene_aware,-,{original_normalize_gene_aware_test_acc},{original_normalize_gene_aware_test_precision},{original_normalize_gene_aware_test_recall},{original_normalize_gene_aware_test_f1},{original_normalize_gene_aware_test_auc}\n')
        f.write(f'{RUN_NAME}_og_esm_no_normalize,-,{original_test_acc},{original_test_precision},{original_test_recall},{original_test_f1},{original_test_auc}\n')
        f.write(f'{RUN_NAME}_og_esm_no_normalize_gene_aware,-,{original_gene_aware_test_acc},{original_gene_aware_test_precision},{original_gene_aware_test_recall},{original_gene_aware_test_f1},{original_gene_aware_test_auc}\n')


    print(f"\nDistance Statistics:")
    print(f"  Mean: {np.mean(final_dists):.4f}")
    print(f"  Std: {np.std(final_dists):.4f}")
    print(f"  Min: {np.min(final_dists):.4f}")
    print(f"  Max: {np.max(final_dists):.4f}")

    print("\n=== Final Distance Clustering ===")
    plot_distance_clustering(final_dists, final_labels, "Final Test Set", final=True)

    print("\n=== Embedding Visualization ===")
    sample_size = min(500, len(test_seqs))
    sample_indices = np.random.choice(len(test_seqs), sample_size, replace=False)

    sample_seqs = [test_seqs[i] for i in sample_indices]
    sample_dms = [test_dms[i] for i in sample_indices]

    sample_embeddings = h5_utils.load_embeddings(args.embeddings_path, sample_seqs)

    visualize_embeddings_tsne(sample_embeddings, sample_dms,
                              f"Original ESM Layer {args.esm_layer} Embeddings (DMS Colored)")

    with torch.no_grad():
        projected_embeddings = projection_net(sample_seqs)

    visualize_embeddings_tsne(projected_embeddings, sample_dms,
                              "Projected Embeddings (DMS Colored)")

    print("\n=== Saving Model ===")
    torch.save({
        'model_state_dict': projection_net.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'train_losses': train_losses,
        'val_losses': val_losses,
        'config': {
            'distance_metric': DISTANCE_METRIC,
            'hidden_dims': HIDDEN_DIMS,
            'learning_rate': LEARNING_RATE,
            'best_val_loss': best_val_loss
        }
    }, f'{RESULTS_DIR}/{RUN_NAME}.pt')
    print(f" Model saved to '{RESULTS_DIR}/{RUN_NAME}.pt'")
    h5_utils.close_h5()

main()

print('Pipeline completed successfully')

"""
import pandas as pd

# Load the data
df = pd.read_csv(DATA_PATH)

print("DMS Score Statistics:")
print(df['DMS_score'].describe())
print(f"\nMin: {df['DMS_score'].min()}")
print(f"Max: {df['DMS_score'].max()}")
print(f"Range: {df['DMS_score'].max() - df['DMS_score'].min()}")

# Check for outliers
print(f"\nValues > 10: {(df['DMS_score'] > 10).sum()}")
print(f"Values > 20: {(df['DMS_score'] > 20).sum()}")
print(f"Values > 40: {(df['DMS_score'] > 40).sum()}")

# Show some high values
print("\nTop 10 highest DMS scores:")
print(df.nlargest(10, 'DMS_score')[['DMS_score', 'filename', 'mutant']])

# Check which genes have extreme DMS scores
extreme_threshold = 10

extreme_rows = df[df['DMS_score'] > extreme_threshold]
print(f"Rows with DMS_score > {extreme_threshold}: {len(extreme_rows)}")
print("\nGenes with extreme values:")
print(extreme_rows.groupby('filename')['DMS_score'].agg(['count', 'min', 'max', 'mean']))

# Show the actual extreme values
print("\nTop 20 highest DMS scores:")
print(df.nlargest(20, 'DMS_score')[['DMS_score', 'filename', 'mutant', 'mutated_sequence']])

"""
